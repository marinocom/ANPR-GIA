{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pytesseract\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure pytesseract is properly configured\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'  # Update the path as needed\n",
    "\n",
    "# Custom Dataset Loading (EMNIST Dataset)\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load EMNIST dataset filtered for uppercase letters and digits\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform)\n",
    "\n",
    "# Filter and remap labels to fit within our 36 classes (0-9, A-Z)\n",
    "filtered_dataset = []\n",
    "for image, label in tqdm(dataset):\n",
    "    if 48 <= label <= 57:  # Digits (0-9)\n",
    "        filtered_label = label - 48  # Remap to 0-9\n",
    "        filtered_dataset.append((image, filtered_label))\n",
    "    elif 65 <= label <= 90:  # Uppercase letters (A-Z)\n",
    "        filtered_label = label - 55  # Remap to 10-35\n",
    "        filtered_dataset.append((image, filtered_label))\n",
    "\n",
    "train_loader = DataLoader(filtered_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the Convolutional Neural Network\n",
    "class CharacterRecognitionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterRecognitionNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 36)  # 36 output classes (10 digits + 26 uppercase letters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 986/986 [00:05<00:00, 185.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 986/986 [00:05<00:00, 186.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.0813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 986/986 [00:05<00:00, 185.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 986/986 [00:05<00:00, 185.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 986/986 [00:05<00:00, 178.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.0423\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Train the Model\n",
    "net = CharacterRecognitionNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "\trunning_loss = 0.0\n",
    "\tfor images, labels in tqdm(train_loader):\n",
    "\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = net(images)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\tprint(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(net.state_dict(), 'character_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116323/116323 [00:19<00:00, 5835.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load EMNIST test dataset filtered for uppercase letters and digits\n",
    "test_dataset = EMNIST(root='./data', split='byclass', train=False, download=True, transform=transform)\n",
    "\n",
    "# Filter and remap labels to fit within our 36 classes (0-9, A-Z)\n",
    "filtered_test_dataset = []\n",
    "for image, label in tqdm(test_dataset):\n",
    "\tif 48 <= label <= 57:  # Digits (0-9)\n",
    "\t\tfiltered_label = label - 48  # Remap to 0-9\n",
    "\t\tfiltered_test_dataset.append((image, filtered_label))\n",
    "\telif 65 <= label <= 90:  # Uppercase letters (A-Z)\n",
    "\t\tfiltered_label = label - 55  # Remap to 10-35\n",
    "\t\tfiltered_test_dataset.append((image, filtered_label))\n",
    "\n",
    "test_loader = DataLoader(filtered_test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113132/2577644477.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('character_recognition_model.pth'))\n",
      "100%|██████████| 163/163 [00:00<00:00, 471.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "net = CharacterRecognitionNet().to(device)\n",
    "net.load_state_dict(torch.load('character_recognition_model.pth'))\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "\tfor images, labels in tqdm(test_loader):\n",
    "\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\toutputs = net(images)\n",
    "\t\t_, predicted = torch.max(outputs.data, 1)\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted character: 2\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APB7CxuNT1G2sLOPzLq6lSGFNwG52ICjJ4GSR1rsItG8D6BqN9pfirUNcu7+1lML/wBjQxLCjqSGAeYhn7D7i4IONwwaw9X0bTbbS7TU9I1qO/t5naKWCdFguoJBz80W9soVwQ6kjOQcEYOHXoHwS/5K9oX/AG8f+k8lHjrRtCtPH3iBtT8R7pJtQnlEWl2huTFucttlLtEoYbgMIXwQwOCOeXm8PXD6Pca1pzfbNKglEUsoAWS3LMwTzY8kpuABBBZMsF3FgQMeug8D+JP+ER8aaXrhj8yO1l/eoFyTGwKPtGR821mxk4zjPFbHinwp4g1rxNqWt6Tot9qOmapdz3lpdWUDTpJE8r4yUB2txyjYZehAq5pF7N4H8B+LNL1zSpIdQ1xLeG0tL23IbapffMUbBULuGxscuBjOxtvndFSQzzWzl4JZInKMhZGKkqylWHHYqSCO4JFR0V//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhElEQVR4AbWRSxKAIAxDq+P9r1wpKWkYxYWjbEjz6E/NfjmbVvUWqCE6UD/ipeNkpRKZOtTHoHKz7A6TNeWNWcLJY/AeTgP1zhyHHwQDwXfiFAHp+ZA1EFlDuVlBLlBiAZG6gMhOWA1hI37MZPccsMeq8YB/cSwim9utx8IhrgUn/E1wArpGGRb6Dn1wAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_image(image_path: str) -> torch.Tensor:\n",
    "\timage = Image.open(image_path).convert('L')  # Convert to grayscale first\n",
    "\timage = ImageOps.invert(image)\n",
    "\timage = image.resize((28, 28))\n",
    "\t\n",
    "\t# Apply binarization\n",
    "\timage = np.array(image)\n",
    "\t_, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\t\n",
    "\t# Apply erosion\n",
    "\tkernel = np.ones((3, 3), np.uint8)\n",
    "\timage = cv2.erode(image, kernel, iterations=1)\n",
    "\t\n",
    "\timage = Image.fromarray(image)\n",
    "\timage = transforms.ToTensor()(image)\n",
    "\timage = transforms.Normalize((0.5,), (0.5,))(image)\n",
    "\treturn image\n",
    "\n",
    "def predict_image(image_path: str, model: nn.Module) -> str:\n",
    "\timage = preprocess_image(image_path).to(device)  # Move image to the same device as the model\n",
    "\timage = image.unsqueeze(0)\n",
    "\toutput = model(image)\n",
    "\t_, predicted = torch.max(output.data, 1)\n",
    "\treturn chr(predicted.item() + 55) if predicted.item() >= 10 else str(predicted.item())\n",
    "\n",
    "image_path = 'example.png'\n",
    "predicted_character = predict_image(image_path, net)\n",
    "print(f'Predicted character: {predicted_character}')\n",
    "\n",
    "def show_image(image_path: str):\n",
    "\t# apply same preprocessing and show\n",
    "\timage = Image.open(image_path).convert('L')\n",
    "\timage = ImageOps.invert(image)\n",
    "\timage = image.resize((28, 28))\n",
    "\t\n",
    "\t# Apply binarization\n",
    "\timage = np.array(image)\n",
    "\t_, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\t\n",
    "\t# Apply erosion\n",
    "\tkernel = np.ones((3, 3), np.uint8)\n",
    "\timage = cv2.erode(image, kernel, iterations=1)\n",
    "\t\n",
    "\timage = Image.fromarray(image)\n",
    "\treturn image\n",
    "\n",
    "show_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted character: 1\n",
      "Correct character: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_image(image: Image.Image) -> torch.Tensor:\n",
    "\t# image = image.convert('L')  # Convert to grayscale first\n",
    "\t# image = ImageOps.invert(image)\n",
    "\t# image = image.resize((28, 28))\n",
    "\t\n",
    "\t# # Apply binarization\n",
    "\t# image = np.array(image)\n",
    "\t# _, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\t\n",
    "\t# # Apply erosion\n",
    "\t# kernel = np.ones((3, 3), np.uint8)\n",
    "\t# image = cv2.erode(image, kernel, iterations=1)\n",
    "\t\n",
    "\t# image = Image.fromarray(image)\n",
    "\timage = transforms.ToTensor()(image)\n",
    "\timage = transforms.Normalize((0.5,), (0.5,))(image)\n",
    "\treturn image\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_image(image: Image.Image, model: nn.Module) -> str:\n",
    "\timage = preprocess_image(image).to(device)  # Move image to the same device as the model\n",
    "\timage = image.unsqueeze(0)\n",
    "\toutput = model(image)\n",
    "\t_, predicted = torch.max(output.data, 1)\n",
    "\treturn chr(predicted.item() + 55) if predicted.item() >= 10 else str(predicted.item())\n",
    "\n",
    "# Get a single image from the test set\n",
    "test_image, test_label = next(iter(test_loader))\n",
    "i = 5\n",
    "\n",
    "# Convert the tensor image to PIL image for display\n",
    "test_image_pil = transforms.ToPILImage()(test_image[i].squeeze(0))\n",
    "\n",
    "# Predict the character\n",
    "predicted_character = predict_image(test_image_pil, net)\n",
    "correct_character = chr(test_label[i].item() + 55) if test_label[i].item() >= 10 else str(test_label[i].item())\n",
    "print(f'Predicted character: {predicted_character}')\n",
    "print(f'Correct character: {correct_character}')\n",
    "\n",
    "# Show the preprocessed image\n",
    "def show_image(image: Image.Image):\n",
    "\t# Apply the same preprocessing and show\n",
    "\t# image = image.convert('L')\n",
    "\t# image = ImageOps.invert(image)\n",
    "\t# image = image.resize((28, 28))\n",
    "\t\n",
    "\t# # Apply binarization\n",
    "\t# image = np.array(image)\n",
    "\t# _, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\t\n",
    "\t# # Apply erosion\n",
    "\t# kernel = np.ones((3, 3), np.uint8)\n",
    "\t# image = cv2.erode(image, kernel, iterations=1)\n",
    "\t\n",
    "\t# image = Image.fromarray(image)\n",
    "\treturn image\n",
    "\n",
    "# Display the original and preprocessed images\n",
    "original_image = transforms.ToPILImage()(test_image[i].squeeze(0))\n",
    "preprocessed_image = show_image(test_image_pil)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.imshow(preprocessed_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:01<00:00, 32.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "# Directory setup for dataset\n",
    "output_dir = \"./custom_dataset\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Path to your .ttf file\n",
    "font_path = \"Datasets/Recognition/Generate Dataset/alte-din-1451-mittelschrift/din1451alt.ttf\"\n",
    "font_size = 70\n",
    "\n",
    "# Load font\n",
    "font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "# Generate images for each character\n",
    "characters = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "for char in tqdm(characters):\n",
    "\tchar_dir = os.path.join(output_dir, char)\n",
    "\tif not os.path.exists(char_dir):\n",
    "\t\tos.makedirs(char_dir)\n",
    "\n",
    "\t# Generate multiple samples for data augmentation\n",
    "\tfor i in range(50):  # Create 50 variants of each character\n",
    "\t\timage = Image.new(\"L\", (64, 64), color=0)  # Black background\n",
    "\t\tdraw = ImageDraw.Draw(image)\n",
    "\t\ttext_bbox = font.getbbox(char)  # Fix: Use font.getbbox instead of font.getsize\n",
    "\n",
    "\t\t# Calculate position to center the character\n",
    "\t\ttext_x = (image.width - (text_bbox[2] - text_bbox[0])) / 2\n",
    "\t\ttext_y = (image.height - (text_bbox[3] - text_bbox[1])) / 2 - text_bbox[1]  # Adjust text_y to center vertically\n",
    "\n",
    "\t\tdraw.text((text_x, text_y), char, fill=255, font=font)  # White character\n",
    "\n",
    "\t\t# Save generated character image\n",
    "\t\timage.save(f\"{char_dir}/{char}_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:02<00:00, 13.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Define augmentations to apply\n",
    "augmentations = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.7),\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.5)\n",
    "])\n",
    "\n",
    "# Load images and apply augmentations\n",
    "for char in tqdm(\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\n",
    "    char_dir = os.path.join(output_dir, char)\n",
    "    image_paths = glob.glob(f\"{char_dir}/*.png\")\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            augmented = augmentations(image=img)\n",
    "            aug_img = augmented[\"image\"]\n",
    "\n",
    "            # Ensure the image is properly inverted and converted to uint8\n",
    "            aug_img = np.clip(aug_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "            # Save augmented images\n",
    "            save_path = image_path.replace(\".png\", \".png\")\n",
    "            cv2.imwrite(save_path, aug_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "class CustomCharDataset(Dataset):\n",
    "\tdef __init__(self, root_dir, transform=None):\n",
    "\t\tself.root_dir = Path(root_dir)\n",
    "\t\tself.transform = transform\n",
    "\t\tself.image_paths = list(self.root_dir.glob('*/*.png'))\n",
    "\t\tself.labels = {char: idx for idx, char in enumerate(characters)}\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.image_paths)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = self.image_paths[idx]\n",
    "\t\timage = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "\t\tlabel = self.labels[img_path.parent.name]\n",
    "\n",
    "\t\tif self.transform:\n",
    "\t\t\timage = self.transform(image)\n",
    "\n",
    "\t\treturn image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize((28, 28)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = CustomCharDataset(root_dir=output_dir, transform=transform)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 2.2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.4147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.2621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.1611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Train the Model\n",
    "net = CharacterRecognitionNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "\trunning_loss = 0.0\n",
    "\tfor images, labels in tqdm(train_loader):\n",
    "\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = net(images)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\tprint(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(net.state_dict(), 'character_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113132/2577644477.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('character_recognition_model.pth'))\n",
      "100%|██████████| 3/3 [00:00<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "net = CharacterRecognitionNet().to(device)\n",
    "net.load_state_dict(torch.load('character_recognition_model.pth'))\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "\tfor images, labels in tqdm(test_loader):\n",
    "\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\toutputs = net(images)\n",
    "\t\t_, predicted = torch.max(outputs.data, 1)\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted character: 6\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APB7CxuNT1G2sLOPzLq6lSGFNwG52ICjJ4GSR1rsItG8D6BqN9pfirUNcu7+1lML/wBjQxLCjqSGAeYhn7D7i4IONwwaw9X0bTbbS7TU9I1qO/t5naKWCdFguoJBz80W9soVwQ6kjOQcEYOHXoHwS/5K9oX/AG8f+k8lHjrRtCtPH3iBtT8R7pJtQnlEWl2huTFucttlLtEoYbgMIXwQwOCOeXm8PXD6Pca1pzfbNKglEUsoAWS3LMwTzY8kpuABBBZMsF3FgQMeug8D+JP+ER8aaXrhj8yO1l/eoFyTGwKPtGR821mxk4zjPFbHinwp4g1rxNqWt6Tot9qOmapdz3lpdWUDTpJE8r4yUB2txyjYZehAq5pF7N4H8B+LNL1zSpIdQ1xLeG0tL23IbapffMUbBULuGxscuBjOxtvndFSQzzWzl4JZInKMhZGKkqylWHHYqSCO4JFR0V//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhElEQVR4AbWRSxKAIAxDq+P9r1wpKWkYxYWjbEjz6E/NfjmbVvUWqCE6UD/ipeNkpRKZOtTHoHKz7A6TNeWNWcLJY/AeTgP1zhyHHwQDwXfiFAHp+ZA1EFlDuVlBLlBiAZG6gMhOWA1hI37MZPccsMeq8YB/cSwim9utx8IhrgUn/E1wArpGGRb6Dn1wAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_image(image_path: str) -> torch.Tensor:\n",
    "\timage = Image.open(image_path).convert('L')  # Convert to grayscale first\n",
    "\timage = ImageOps.invert(image)\n",
    "\timage = image.resize((28, 28))\n",
    "\t\n",
    "\t# Apply binarization\n",
    "\timage = np.array(image)\n",
    "\t_, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\t\n",
    "\t# Apply erosion\n",
    "\tkernel = np.ones((3, 3), np.uint8)\n",
    "\timage = cv2.erode(image, kernel, iterations=1)\n",
    "\t\n",
    "\timage = Image.fromarray(image)\n",
    "\timage = transforms.ToTensor()(image)\n",
    "\timage = transforms.Normalize((0.5,), (0.5,))(image)\n",
    "\treturn image\n",
    "\n",
    "def predict_image(image_path: str, model: nn.Module) -> str:\n",
    "\timage = preprocess_image(image_path).to(device)  # Move image to the same device as the model\n",
    "\timage = image.unsqueeze(0)\n",
    "\toutput = model(image)\n",
    "\t_, predicted = torch.max(output.data, 1)\n",
    "\treturn chr(predicted.item() + 55) if predicted.item() >= 10 else str(predicted.item())\n",
    "\n",
    "image_path = 'example.png'\n",
    "predicted_character = predict_image(image_path, net)\n",
    "print(f'Predicted character: {predicted_character}')\n",
    "\n",
    "def show_image(image_path: str):\n",
    "\t# apply same preprocessing and show\n",
    "\timage = Image.open(image_path).convert('L')\n",
    "\timage = ImageOps.invert(image)\n",
    "\timage = image.resize((28, 28))\n",
    "\t\n",
    "\t# Apply binarization\n",
    "\timage = np.array(image)\n",
    "\t_, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\t\n",
    "\t# Apply erosion\n",
    "\tkernel = np.ones((3, 3), np.uint8)\n",
    "\timage = cv2.erode(image, kernel, iterations=1)\n",
    "\t\n",
    "\timage = Image.fromarray(image)\n",
    "\treturn image\n",
    "\n",
    "show_image(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
